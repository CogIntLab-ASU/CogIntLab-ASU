
<!DOCTYPE HTML>
<!--
 Monochromed by TEMPLATED
 templated.co @templatedco
 Released for free under the Creative Commons Attribution 3.0 license (templated.co/license)
 -->
<html>
    <head>

        <title>Cognition and Intelligence Lab @ASU</title>
        <meta http-equiv="content-type" content="text/html; charset=utf-8" />
        <meta name="description" content="" />
        <meta name="keywords" content="" />
        <link href='http://fonts.googleapis.com/css?family=Oxygen:400,300,700' rel='stylesheet' type='text/css'>
            <!--[if lte IE 8]><script src="js/html5shiv.js"></script><![endif]-->
            <script src="http://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
            <script src="js/skel.min.js"></script>
            <script src="js/skel-panels.min.js"></script>
            <script src="js/init.js"></script>
            <noscript>
            <link rel="stylesheet" href="css/skel-noscript.css" />
            <link rel="stylesheet" href="css/style.css" />
            </noscript>
            </head>
            
            <body class="homepage">
            <div id="header">
                  <div class="container">
                              
                        <!-- Logo -->
                              <div id="logo">
                                    <h1><a href="#" style="color: goldenrod;">Cognition & Intelligence Lab</a></h1>
                                    <span style="color: lightgoldenrodyellow;">@Arizona State University</span>
                              </div>
                        
                        <!-- Nav -->
                              <nav id="nav">
                                    <ul>
                                          <li class="active"><a href="index.html">Homepage</a></li>
                                          <li><a href="research.html">Research</a></li>
                                          <li><a href="team.html">Team</a></li>
                                          <li><a href="publications.html">Publications</a></li>
                                          <li><a href="https://github.com/CogIntLab-ASU/">software</a></li>
                                          <li><a href="photos.html">Photos</a></li>
                                    </ul>
                              </nav>

                  </div>
            </div>
            <!-- Header -->
            
            <!-- Main -->
            <div id="main">
            <div class="container">
             
            <header>
              <h2 align="center">Introduction to our Lab</h2><br>
            </header>
             The current research focus of our lab is to build automated systems (and develop underlying methodologies 
             and address associated challenges) that can understand text, images, and videos and apply that understanding 
             to various AI (Artificial Intelligence), Robotics and Human Centered AI domains such as teaching robots 
             through demonstrations and language instructions, assisting health care, enabling scientific discovery 
             through automated literature processing, and man-machine collaboration on difficult tasks such as 
             software vulnerability detection and human robot collaboration.
<br>
<br>
             Machine Learning (and in recent years the deep learning approach) is a key method that is 
             often used for the above. Our lab's focus and USP is to augment machine learning (including deep learning 
             approaches) with knowledge and reasoning for the above tasks, as often times there exists accumulated 
             task-relevant knowledge and often it is important to use commonsense reasoning. 
             In pursuing the use of knowledge and reasoning together with machine learning we are 
             faced with several questions and challenges such as: how to incorporate knowledge and 
             reasoning into machine learning methods; how to acquire knowledge, especially commonsense 
             knowledge; what are the key aspects of commonsense knowledge; what knowledge representation 
             formalisms to use;  how to figure out what knowledge is missing; how to obtain knowledge from 
             text; what knowledge learning approach to use; how to use question answering datasets to 
             acquire knowledge; how to use crowdsourcing for knowledge acquisition; and how to do 
             reasoning in the face of mistake prone knowledge extraction methods and in the absence of a 
             unified knowledge representation formalism.  
<br>
<br>
             Our research falls under the general area of AI but currently has a special focus on Cognition. 
             (Oxford dictionary defines intelligence as "the ability to acquire and apply knowledge and skills" 
             and defines Cognition as "the mental action or process of acquiring knowledge and understanding 
             through thought, experience, and the senses.") Hence the name of our lab. 
<br>

            
            <div class="publicationContent">

                <div class="paperBlock">
                    <span class="paperTitle">Cognition: Natural Language Understanding & Question Answering</span><br/>
                    <a href=""><img src="images/pubpics/nlu.jpg" /></a>
                    <br>
                    <span class="authorList">
                      <ul>
                        <li> Arindam Mitra, Ishan Shrivastava and Chitta Baral. Understanding Roles and Entities: Datasets and Models for Natural Language Inference. <a href="https://arxiv.org/abs/1904.09720">https://arxiv.org/abs/1904.09720</a> </li>
                        <li>Ashok Prakash, Arpit Sharma, Arindam Mitra and Chitta Baral. Combining Knowledge Hunting and Neural Language Models to Solve the Winograd Schema Challenge. ACL 2019.</li>
                        <li>Pratyay Banerjee, Kuntal Kumar Pal, Arindam Mitra and Chitta Baral. Careful Selection of Knowledge to solve Open Book Question Answering. ACL 2019.</li>
                        <li>Arindam Mitra, Peter Clark, Oyvind Tafjord and Chitta Baral. Declarative Question Answering over Knowledge Bases containing Natural Language Text with Answer Set Programming. AAAI 2019.</li>
                        <li>Arindam Mitra and Chitta Baral. Learning to use formulas to solve simple arithmetic problems. ACL 2016.</li>
                        <li>Arindam Mitra and Chitta Baral. Addressing a Question Answering Challenge by Combining Statistical Methods with Inductive Rule Learning and Reasoning. AAAI 2016</li>
                        <li>Vo Nguyen, Arindam Mitra and Chitta Baral. The NL2KR platform for building Natural Language Translation Systems. ACL 2015.</li>
                      </ul>

                    </span>
                    <div class="venue"></div>
                </div>


                <div class="paperBlock">
                    <span class="paperTitle">Cognition: Image and multi-modal document Understanding and Visual QA</span><br/>
                    
                    <br>
                    <span class="authorList">
                        <a href=""><img src="images/pubpics/tgokhale_blocks.png" width="50" /></a>
                        <li>
                          <a href="https://arxiv.org/pdf/1905.12042.pdf" style="color:tomato; text-decoration:none;">Blocksworld Revisited: Learning and Reasoning to Generate Event-Sequences from Image Pairs. </a>
                          <br>
                          <font color="mediumpurple">Gokhale, Sampat, Fang, Yang, Baral. </font> 
                          <br>
                          <font color="limegreen">Preprint, <a href="http://openaccess.thecvf.com/content_CVPRW_2019/papers/Vision_Meets_Cognition_Camera_Ready/Gokhale_Cooking_With_Blocks__A_Recipe_for_Visual_Reasoning_on_CVPRW_2019_paper.pdf" style="color:limegreen; text-decoration:none;">CVPR Workshop Version</a></font>
                        </li>
                        <a href=""><img src="images/pubpics/somak_survey.png" width="50" /></a>
                        <li>
                          <a href="https://www.ijcai.org/proceedings/2019/0873.pdf" style="color:tomato; text-decoration:none;">Integrating Knowledge and Reasoning in Image Understanding. </a>
                          <br>
                          <font color="mediumpurple">Aditya, Yang, Baral </font> 
                          <br>
                          <font color="limegreen">IJCAI 2019</font>
                        </li>

                        <li>
                          <a href="https://www.public.asu.edu/~cbaral/papers/2019-wacv.pdf" style="color:tomato; text-decoration:none;">Spatial Knowledge Distillation to aid Visual Reasoning.</a>
                          <br>
                          <font color="mediumpurple">Aditya, Saha, Yang, Baral </font> 
                          <br>
                          <font color="limegreen">WACV 2019</font>
                        </li>

                        <li>
                          <a href="https://www.public.asu.edu/~cbaral/papers/2018-aaai-psl.pdf" style="color:tomato; text-decoration:none;">Explicit Reasoning over End-to-End Neural Architectures for Visual Question Answering.</a>
                          <br>
                          <font color="mediumpurple">Aditya, Yang, Baral </font> 
                          <br>
                          <font color="limegreen">AAAI 2018</font>
                        </li>

                        <li>
                          <a href="http://auai.org/uai2018/proceedings/papers/83.pdf" style="color:tomato; text-decoration:none;">Combining Knowledge and Reasoning through Probabilistic Soft Logic for Image Puzzle Solving.</a>
                          <br>
                          <font color="mediumpurple">Aditya, Yang, Baral, Aloimonos </font> 
                          <br>
                          <font color="limegreen">UAI 2018</font>
                        </li>

                        <li>
                          <a href="https://www.sciencedirect.com/science/article/pii/S1077314217302291" style="color:tomato; text-decoration:none;">Image Understanding using Vision and Reasoning through Scene Description Graph.</a>
                          <br>
                          <font color="mediumpurple">Aditya, Yang, Baral, Aloimonos, Fermuller </font> 
                          <br>
                          <font color="limegreen">Computer Vision and Image Understanding Journal. Dec 2017</font>
                        </li>

                        <li>
                          <a href="http://www.cogsys.org/papers/ACS2016/Papers/Aditya_et.al-ACS-2016.pdf" style="color:tomato; text-decoration:none;">DeepIU: An architecture for image understanding.</a>
                          <br>
                          <font color="mediumpurple">Aditya, Baral, Yang, Aloimonos, Fermuller </font> 
                          <br>
                          <font color="limegreen">Advances in Cognitive Systems. 2016.</font>
                        </li>
                      
                        
                        

                      </ul>
                    </span>
                    <div class="venue"></div>
                </div>


                <div class="paperBlock">
                    <span class="paperTitle">Human-Centered AI: </span><br/>
                    <a href=""><img src="images/pubpics/hcai.jpg" /></a>
                    <span class="authorList"></span>
                    <div class="venue"></div>
                </div>


                <div class="paperBlock">
                    <span class="paperTitle">AI Foundations</span><br/>
                    <a href=""><img src="images/pubpics/ai.jpg" /></a>
                    <span class="authorList"> </span>
                    <div class="venue"> </div>
                </div>  
            
            </div>

            
            </div>
            
            
            
            
            </div>
            <!-- Main -->
            
            <!-- Footer -->
            <div id="footer">
            
            <div class="container">
            
            <div class="row">
            
            
            <!-- Sidebar -->
            <div class="5u">
            <section>
            <header>
            <h2>News</h2>
            </header>
            <!-- <ul class="news">
            <li><strong>08/18/2019</strong> Byeongjoo's paper on <a href=files/paper/2019/NLOSConv_ICCV19.pdf>convolutational approximations to the NLOS imaging operator</a> to be presented as an oral at ICCV 2019.
            <li><strong>06/18/2019</strong> Fermat Paths wins the <a href=http://cvpr2019.thecvf.com/program/main_conference#awards>best paper award</a> at <a href=http://cvpr2019.thecvf.com/>CVPR 2019</a>!
            <li><strong>05/15/2019</strong> PhaseCam3D wins the <a href=http://iccp2019.naist.jp/images/awards/poster0001.jpg>best poster award</a> at <a href=http://iccp2019.naist.jp/>ICCP 2019</a>!
            <li><strong>04/05/2019</strong> Three papers to appear in CVPR2019: 1) <a href=files/paper/2019/Fermat_CVPR19.pdf>Fermat paths for NLOS reconstructions</a>, 2)  <a href=files/paper/2019/NLOSInvRender_CVPR19.pdf>Surface optimization for NLOS</a>, and 3) <a href=files/paper/2019/IllumSep_CVPR19.pdf>Illuminant separation using deep learning</a>
            <li><strong>04/05/2019</strong> Two papers to appear in ICCP2019: 1) <a href=files/paper/2019/WaveletTree_ICCP19.pdf>Wavelet trees and adaptive illumination</a>, and 2)  <a href=files/paper/2019/PhaseCam_ICCP19.pdf>Optimized  phase masks for passive 3D</a>
            <li><strong>01/31/2019</strong> Chia-Yin Tsai defends. Congratulations!
            <li><strong>01/23/2019</strong> Rick's VR prototype covered in the press. <a href=https://techxplore.com/news/2019-01-d-accommodate-human-eye.html>(techxplore)</a> <a href=https://uploadvr.com/multifocal-1600-fps/>(uploadvr)</a>
            <li><strong>01/22/2019</strong> Harry Hui defends. Congratulations!
            <li><strong>01/10/2019</strong> Vishwa was  selected to receive the Prabhu and Poonam Goel Graduate Fellowship for the 2018/2019 academic year!
            <li><strong>10/04/2018</strong> Rick presents a  <a href=http://imagesci.ece.cmu.edu/files/paper/2018/VR_SA18.pdf>new VR prototype</a> at SIGGRAPH Asia
            <li><strong>09/18/2018</strong> Jian's paper on <a href=http://imagesci.ece.cmu.edu/files/paper/2018/LC_ECCV18.pdf>programmable light curtains</a> will be presented as an oral in ECCV 2018!
                <li><strong>07/26/2018</strong> ISL#01 --- Jian defends! A doctor is made. Congratulations! You can be the first to read his dissertation <a href=http://imagesci.ece.cmu.edu/files/paper/2018/JWthesis_v2.pdf>here</a>
                <li><strong>06/19/2018</strong> Harry's paper on <a href=http://imagesci.ece.cmu.edu/files/paper/2018/LightEditing_CVPR18.pdf>illumination editing</a> to be presented as an oral at <a href="http://cvpr2018.thecvf.com/">CVPR</a>. Congratulations, Harry!</li>
                <li><strong>05/16/2018</strong> A number of awards won by group members at ECE commencement ceremony. Vishwa --- the <b>Outstanding TA award</b>, Yongyi --- <b>Frank J. Marshall Scholar Award</b>, Biqi --- <b>Outstanding Woman in Engineering Award</b>, and Aswin --- the <b>Joel and Rut Spira Excellence in Teaching Award</b> as part of the 18-290 instructor team.</li>


                <li><strong>05/04/2018</strong> <a href=http://iccp2018.ece.cmu.edu>ICCP 2018</a> will be held in CMU from May 4-6th. Follow us on <a href=https://twitter.com/iccp_conference>twitter</a> and <a href= https://www.facebook.com/ieeeiccp/>FB</a>.</li>
                 <li><strong>09/22/2017</strong> Rick's paper on <a href=files/paper/2017/OneNet_ICCV17.pdf>One Network to Solve Them All</a> to be presented as an oral at <a href=http://iccv2017.thecvf.com>ICCV</a>. Congratulations, Rick!.</li>
                 <li><strong>09/22/2017</strong> Harry and Jian's' paper on <a href=files/paper/2017/Univariate_ICCV17.pdf>univariate sampling of BRDFs</a> to be presented at <a href=http://iccv2017.thecvf.com>ICCV</a>. Congratulations!</li>
                 <li><strong>05/22/2017</strong> Aswin awarded the Eta Kappa Nu (CMU chapter) Excellence in Teaching Award for 2017.</li>
                 <li><strong>04/10/2017</strong> Chia-Yin's paper on <a href=files/paper/2017/NLOS_CVPR17.pdf>non-line-of-sight imaging</a> to be presented as a spotlight at <a href=http://cvpr2017.thecvf.com>CVPR</a>. Congratulations, Chia-Yin!.</li>
                 <li><strong>03/19/2017</strong> Vishwa's paper on <a href=files/paper/2017/Anomaly_ICCP17.pdf>spectral anomaly detection</a> to be presented at <a href=http://iccp2017.stanford.edu>ICCP</a>. Congratulations, Vishwa!.</li>
                 <li><strong>02/08/2017</strong> Aswin wins a 2017 NSF CAREER award for <a href=http://www.ece.cmu.edu/news/story/2017/02/sankaranarayanan-receives-nsf-career-grant-to-study-light-interactions.html>plenoptic signal processing</a>.</li>
                 <li><strong>01/10/2017</strong> We are co-organizing the <a href=http://ccdworkshop.org>CCD workshop</a>.
                 <li><strong>12/08/2016</strong> Rick and Jian win the CIT Bertucci and Ji Dian fellowships, respectively.  Congratulations!</li>
                 <li><strong>11/08/2016</strong> Paper on <a href=files/paper/2016/HDRProjector_OpEx16.pdf>high bit-depth projection</a> accepted at Optics Express. Congratulations, Rick!</li>
                 <li><strong>10/25/2016</strong> Paper on <a href=files/paper/2017/virtualex_pami17.pdf>photometric stereo for spatially-varying BRDF</a> accepted at PAMI. Congratulations, Harry!</li>
                 <li><strong>07/25/2016</strong> Paper on <a href=files/paper/2016/DualSL_ECCV16.pdf>Dual Structured Light</a> accepted at ECCV 2016. Congratulations, Jian!</li>
                 <li><strong>05/05/2016</strong> Paper on <a href=files/paper/2016/crossscale_ICIP16.pdf>cross-scale dictionaries</a> accepted at ICIP 2016. Congratulations, Vishwa!</li>
                 <li><strong>04/15/16</strong> Aswin (along with FlatCam team) wins the 2016 Herschel M. Rich award from Rice University for work on FlatCam!</li>
                 <li><strong>03/09/16</strong> Chia-Yin recieves the <a href=http://googleresearch.blogspot.com/2016/03/announcing-2016-google-phd-fellows-for.html>Google PhD Fellowship!</a> Congratulations!!</li>
                 <li><strong>03/05/16</strong> Three accepted papers: one in <a href=http://cvpr2016.thecvf.com/>CVPR 2016</a> and two in <a href=http://www.iccp16.org/>ICCP 2016</a>. Congrats to Rick, Harry, and Chia-Yin.</li>
                 <li><strong>02/26/16</strong> We are co-organizing the CVPR 2016 Workshop on <a href="http://www.ccdworkshop.org">Computational Cameras and Displays</a></li>
                 
                 <li><strong>10/05/15</strong> Paper on lensless imaging accepted at ICCVW</li>
                 <li><strong>09/25/15</strong> Paper on small baseline photometric stereo accepted at ICCV</li>
                 <li><strong>09/25/15</strong> Chia-Yin selected as a finalist in <a href=http://www.icip2015.org/students.html>3MT video competition</a> at ICIP</li>
                 <li><strong>09/25/15</strong> Chia-Yin presents paper on transparent shape reconstruction at ICIP</li>
                 <li><strong>06/10/15</strong> Best paper award in CCD workshop</li>
                 <li><strong>06/10/15</strong> Aswin presents keynote talk in CCD workshop</li>
                 <li><strong>06/10/15</strong> Paper on SWIR video CS presented at CVPR</li>
                 S
                 <li><strong>04/26/15</strong> Jian and Harry present their papers in ICCP</li>
                 <li><strong>04/26/15</strong> Best poster runner-up at ICCP</li>
                 --->
                </ul> -->
                
                </section>
                </div>
                <!-- Sidebar -->
                <div class="1u">
                    &nbsp;
                </div>
                <div class="6u">
                    <section>
                        <header>
                            <!-- <h2><a href=https://www.youtube.com/playlist?list=PLLvWermX0HilUM0LZDuviqoOg1oDzWM_S>Video playlist</a></h2> -->
                        </header>
                        <!-- <iframe  src="https://www.youtube.com/embed/videoseries?list=PLLvWermX0HilUM0LZDuviqoOg1oDzWM_S" frameborder="0" width="540" height="340" allowfullscreen></iframe> -->
                        
                    </section>
                    <br>
                    <section id="sidebar">
                        <header>
                            <h2>Contact</h2>
                        </header>
                        <p>
                        <strong>Address</strong><br>
                        Brickyard Engineering 572<br>
                        CIDSE, ASU<br>
                        699 S Mill Ave, <br>
                        Tempe AZ 85281<br>
                        <br>
                        <strong>Email</strong><br> chitta@asu.edu<br>
                        <br>
                        <strong>Phone</strong><br> <br>

                        </p>
                    </section>
                </div>
                
                <!-------
                 <div class="4u">
                 <section>
                 <header>
                 <h2>Press</h2>
                 </header>
                 <ul class="news">
                 <li><strong>November, 2015</strong><br>
                 FlatCam covered at multiple venues<br>
                 <a href=http://www.nbcnews.com/tech/innovation/flatcam-skips-lens-camera-thinner-dime-n468326>NBC News<a>
                 &nbsp;<a href=http://www.futurity.org/flatcam-camera-lenses-1054912-2/>Futurity.org</a>
                 &nbsp;<a href=http://phys.org/news/2015-11-lens-problem-flatcam.html>Phys.org</a><br>
                 <a href=http://image-sensors-world.blogspot.com/2015/11/rice-university-to-present-lensless.html>Image-Sensors-World</a>
                 </li>
                 
                 <br><li><strong>October, 2015</strong><br><a href=http://charge.ece.ncsu.edu/2015/10/carnegie-mellon-team-develops-camera-that-uses-sensors-with-just-1000-pixels/>LiSens covered in Charge @NCSU</a></li>
                 <br><li><strong>June 4, 2015</strong> <br><a href=http://3dprintingindustry.com/2015/06/04/the-future-of-3d-scanning-the-latest-research-from-carnegie-mellon-university/>The future of 3D scanning ... </a></li>
                 <br><li><strong>June 2, 2015</strong> <br><a href=http://phys.org/news/2015-06-cmu-d-scanning-technology-interaction.html>CMU researchers develop 3D scanning tech...</a></li>
                 <br><li><strong>June 2, 2015</strong> <br><a href=http://phys.org/news/2015-06-team-camera-sensors-pixels.html>Team develops camera that uses sensor...</a></li>
                 
                 </ul>
                 </section>
                 </div>
                 --->
                
                </div>
                </div>
                </div>
                <!-- Footer -->
                
                <!-- Copyright -->
                <div id="copyright">
                    <div class="container">
                        Design: <a href="http://templated.co">TEMPLATED</a><br>Header Image: <a href="images/byeng.jpg">Rufus2010</a>.
                        <i><a rel="nofollow" class="external text" href="http://creativecommons.org/licenses/by-sa/2.5/">(CC Attribution-ShareAlike 2.5)</a></i>
                    </div>
                </div>
                
                </body>
                </html>
