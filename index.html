
<!DOCTYPE HTML>
<!--
 Monochromed by TEMPLATED
 templated.co @templatedco
 Released for free under the Creative Commons Attribution 3.0 license (templated.co/license)
 -->
<html>
    <head>

        <title>Cognition and Intelligence Lab @ASU</title>
        <meta http-equiv="content-type" content="text/html; charset=utf-8" />
        <meta name="description" content="" />
        <meta name="keywords" content="" />
        <link href='http://fonts.googleapis.com/css?family=Oxygen:400,300,700' rel='stylesheet' type='text/css'>
            <!--[if lte IE 8]><script src="js/html5shiv.js"></script><![endif]-->
            <script src="http://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
            <script src="js/skel.min.js"></script>
            <script src="js/skel-panels.min.js"></script>
            <script src="js/init.js"></script>
            <noscript>
            <link rel="stylesheet" href="css/skel-noscript.css" />
            <link rel="stylesheet" href="css/style.css" />
            </noscript>
            </head>
            
            <body class="homepage">
            <div id="header">
                  <div class="container">
                              
                        <!-- Logo -->
                              <div id="logo">
                                    <h1><a href="#" style="color: goldenrod;">Cognition & Intelligence Lab</a></h1>
                                    <span style="color: lightgoldenrodyellow;">@Arizona State University</span>
                              </div>
                        
                        <!-- Nav -->
                              <nav id="nav">
                                    <ul>
                                          <li class="active"><a href="index.html">Homepage</a></li>
                                          <li><a href="research.html">Research</a></li>
                                          <li><a href="team.html">Team</a></li>
                                          <li><a href="publications.html">Publications</a></li>
                                          <li><a href="https://github.com/CogIntLab-ASU/">software</a></li>
                                          <li><a href="photos.html">Photos</a></li>
                                    </ul>
                              </nav>

                  </div>
            </div>
            <!-- Header -->
            
            <!-- Main -->
            <div id="main">
            <div class="container">
             
            <header>
              <h2 align="center">Introduction to our Lab</h2><br>
            </header>
             The current research focus of our lab is to build automated systems (and develop underlying methodologies 
             and address associated challenges) that can understand text, images, and videos and apply that understanding 
             to various AI (Artificial Intelligence), Robotics and Human Centered AI domains such as teaching robots 
             through demonstrations and language instructions, assisting health care, enabling scientific discovery 
             through automated literature processing, and man-machine collaboration on difficult tasks such as 
             software vulnerability detection and human robot collaboration.
<br>
<br>
             Machine Learning (and in recent years the deep learning approach) is a key method that is 
             often used for the above. Our lab's focus and USP is to augment machine learning (including deep learning 
             approaches) with knowledge and reasoning for the above tasks, as often times there exists accumulated 
             task-relevant knowledge and often it is important to use commonsense reasoning. 
             In pursuing the use of knowledge and reasoning together with machine learning we are 
             faced with several questions and challenges such as: how to incorporate knowledge and 
             reasoning into machine learning methods; how to acquire knowledge, especially commonsense 
             knowledge; what are the key aspects of commonsense knowledge; what knowledge representation 
             formalisms to use;  how to figure out what knowledge is missing; how to obtain knowledge from 
             text; what knowledge learning approach to use; how to use question answering datasets to 
             acquire knowledge; how to use crowdsourcing for knowledge acquisition; and how to do 
             reasoning in the face of mistake prone knowledge extraction methods and in the absence of a 
             unified knowledge representation formalism.  
<br>
<br>
             Our research falls under the general area of AI but currently has a special focus on Cognition. 
             (Oxford dictionary defines intelligence as "the ability to acquire and apply knowledge and skills" 
             and defines Cognition as "the mental action or process of acquiring knowledge and understanding 
             through thought, experience, and the senses.") Hence the name of our lab. 
<br>
<br>
<br>

            
            <div class="publicationContent">

                <div class="paperBlock">
                    <span class="paperTitle">Cognition: Natural Language Understanding & Question Answering</span><br/>
                    <a href=""><img src="images/pubpics/nlu.jpg" /></a>
                    <br>
                    <span class="authorList">
                      <ul>
                        <li> Arindam Mitra, Ishan Shrivastava and Chitta Baral. Understanding Roles and Entities: Datasets and Models for Natural Language Inference. <a href="https://arxiv.org/abs/1904.09720">https://arxiv.org/abs/1904.09720</a> </li>
                        <li>Ashok Prakash, Arpit Sharma, Arindam Mitra and Chitta Baral. Combining Knowledge Hunting and Neural Language Models to Solve the Winograd Schema Challenge. ACL 2019.</li>
                        <li>Pratyay Banerjee, Kuntal Kumar Pal, Arindam Mitra and Chitta Baral. Careful Selection of Knowledge to solve Open Book Question Answering. ACL 2019.</li>
                        <li>Arindam Mitra, Peter Clark, Oyvind Tafjord and Chitta Baral. Declarative Question Answering over Knowledge Bases containing Natural Language Text with Answer Set Programming. AAAI 2019.</li>
                        <li>Arindam Mitra and Chitta Baral. Learning to use formulas to solve simple arithmetic problems. ACL 2016.</li>
                        <li>Arindam Mitra and Chitta Baral. Addressing a Question Answering Challenge by Combining Statistical Methods with Inductive Rule Learning and Reasoning. AAAI 2016</li>
                        <li>Vo Nguyen, Arindam Mitra and Chitta Baral. The NL2KR platform for building Natural Language Translation Systems. ACL 2015.</li>
                      </ul>

                    </span>
                    <div class="venue"></div>
                </div>


                <div class="paperBlock">
                    <span class="paperTitle">Cognition: Image and multi-modal document Understanding and Visual QA</span><br/>
                    
                    <br>
                    <span class="authorList">
                        <a href=""><img src="images/pubpics/tgokhale_blocks.PNG" width="50" /></a>
                        <li>
                          <a href="https://arxiv.org/pdf/1905.12042.pdf" style="color:tomato; text-decoration:none;">Blocksworld Revisited: Learning and Reasoning to Generate Event-Sequences from Image Pairs. </a>
                          <br>
                          <font color="mediumpurple">Gokhale, Sampat, Fang, Yang, Baral. </font> 
                          <br>
                          <font color="limegreen">Preprint, <a href="http://openaccess.thecvf.com/content_CVPRW_2019/papers/Vision_Meets_Cognition_Camera_Ready/Gokhale_Cooking_With_Blocks__A_Recipe_for_Visual_Reasoning_on_CVPRW_2019_paper.pdf" style="color:limegreen; text-decoration:none;">CVPR Workshop Version</a></font>
                        </li>
                        <a href=""><img src="images/pubpics/somak_survey.PNG" width="50" /></a>
                        <li>
                          <a href="https://www.ijcai.org/proceedings/2019/0873.pdf" style="color:tomato; text-decoration:none;">Integrating Knowledge and Reasoning in Image Understanding. </a>
                          <br>
                          <font color="mediumpurple">Aditya, Yang, Baral </font> 
                          <br>
                          <font color="limegreen">IJCAI 2019</font>
                        </li>
                        <a href=""><img src="images/pubpics/saha_mask.PNG" width="50" /></a>
                        <li>

                          <a href="https://www.public.asu.edu/~cbaral/papers/2019-wacv.pdf" style="color:tomato; text-decoration:none;">Spatial Knowledge Distillation to aid Visual Reasoning.</a>
                          <br>
                          <font color="mediumpurple">Aditya, Saha, Yang, Baral </font> 
                          <br>
                          <font color="limegreen">WACV 2019</font>
                        </li>

                        <li>
                          <a href="https://www.public.asu.edu/~cbaral/papers/2018-aaai-psl.pdf" style="color:tomato; text-decoration:none;">Explicit Reasoning over End-to-End Neural Architectures for Visual Question Answering.</a>
                          <br>
                          <font color="mediumpurple">Aditya, Yang, Baral </font> 
                          <br>
                          <font color="limegreen">AAAI 2018</font>
                        </li>

                        <li>
                          <a href="http://auai.org/uai2018/proceedings/papers/83.pdf" style="color:tomato; text-decoration:none;">Combining Knowledge and Reasoning through Probabilistic Soft Logic for Image Puzzle Solving.</a>
                          <br>
                          <font color="mediumpurple">Aditya, Yang, Baral, Aloimonos </font> 
                          <br>
                          <font color="limegreen">UAI 2018</font>
                        </li>

                        <li>
                          <a href="https://www.sciencedirect.com/science/article/pii/S1077314217302291" style="color:tomato; text-decoration:none;">Image Understanding using Vision and Reasoning through Scene Description Graph.</a>
                          <br>
                          <font color="mediumpurple">Aditya, Yang, Baral, Aloimonos, Fermuller </font> 
                          <br>
                          <font color="limegreen">Computer Vision and Image Understanding Journal. Dec 2017</font>
                        </li>

                        <li>
                          <a href="http://www.cogsys.org/papers/ACS2016/Papers/Aditya_et.al-ACS-2016.pdf" style="color:tomato; text-decoration:none;">DeepIU: An architecture for image understanding.</a>
                          <br>
                          <font color="mediumpurple">Aditya, Baral, Yang, Aloimonos, Fermuller </font> 
                          <br>
                          <font color="limegreen">Advances in Cognitive Systems. 2016.</font>
                        </li>
                      
                        
                        

                      </ul>
                    </span>
                    <div class="venue"></div>
                </div>


                <div class="paperBlock">
                    <span class="paperTitle">Human-Centered AI: </span><br/>
                    <a href=""><img src="images/pubpics/hcai.jpg" /></a>
                    <span class="authorList"></span>
                    <div class="venue"></div>
                </div>


                <div class="paperBlock">
                    <span class="paperTitle">AI Foundations</span><br/>
                    <a href=""><img src="images/pubpics/ai.jpg" /></a>
                    <span class="authorList"> </span>
                    <div class="venue"> </div>
                </div>  
            
            </div>

            
            </div>
            
            
            
            
            </div>
            <!-- Main -->
            
            <!-- Footer -->
            <div id="footer">
            
            <div class="container">
            
            <div class="row">
            
            
            <!-- Sidebar -->
            <div class="5u">
            <section>
            <header>
            <h2>News</h2>
            </header>
            <ul class="news">
              <li><strong>Sept 2019</strong> Arindam Mitra graduates!</li>
              <li><strong>Aug 2019</strong> Shailaja, Tejas talk at IJCAI Doctoral Consortium</li>
              <li><strong>June 2019</strong> Arpit Sharma graduates!</li>
              <li><strong>May 2019</strong> 3 papers in ACL 2019</li>
              <li><strong>May 2019</strong> 1 paper in IJCAI 2019</li>
          



          
            </ul>
                
                </section>
                </div>
                <!-- Sidebar -->
                <div class="1u">
                    &nbsp;
                </div>
                <div class="6u">
                    <section>
                        <header>
                            <!-- <h2><a href=https://www.youtube.com/playlist?list=PLLvWermX0HilUM0LZDuviqoOg1oDzWM_S>Video playlist</a></h2> -->
                        </header>
                        <!-- <iframe  src="https://www.youtube.com/embed/videoseries?list=PLLvWermX0HilUM0LZDuviqoOg1oDzWM_S" frameborder="0" width="540" height="340" allowfullscreen></iframe> -->
                        
                    </section>
                    <br>
                    <section id="sidebar">
                        <header>
                            <h2>Contact</h2>
                        </header>
                        <p>
                        <strong>Address</strong><br>
                        Brickyard Engineering 572<br>
                        CIDSE, ASU<br>
                        699 S Mill Ave, <br>
                        Tempe AZ 85281<br>
                        <br>
                        <strong>Email</strong><br> chitta@asu.edu<br>
                        <br>
                        <strong>Phone</strong><br> <br>

                        </p>
                    </section>
                </div>
                
                <!-------
                 <div class="4u">
                 <section>
                 <header>
                 <h2>Press</h2>
                 </header>
                 <ul class="news">
                 <li><strong>November, 2015</strong><br>
                 FlatCam covered at multiple venues<br>
                 <a href=http://www.nbcnews.com/tech/innovation/flatcam-skips-lens-camera-thinner-dime-n468326>NBC News<a>
                 &nbsp;<a href=http://www.futurity.org/flatcam-camera-lenses-1054912-2/>Futurity.org</a>
                 &nbsp;<a href=http://phys.org/news/2015-11-lens-problem-flatcam.html>Phys.org</a><br>
                 <a href=http://image-sensors-world.blogspot.com/2015/11/rice-university-to-present-lensless.html>Image-Sensors-World</a>
                 </li>
                 
                 <br><li><strong>October, 2015</strong><br><a href=http://charge.ece.ncsu.edu/2015/10/carnegie-mellon-team-develops-camera-that-uses-sensors-with-just-1000-pixels/>LiSens covered in Charge @NCSU</a></li>
                 <br><li><strong>June 4, 2015</strong> <br><a href=http://3dprintingindustry.com/2015/06/04/the-future-of-3d-scanning-the-latest-research-from-carnegie-mellon-university/>The future of 3D scanning ... </a></li>
                 <br><li><strong>June 2, 2015</strong> <br><a href=http://phys.org/news/2015-06-cmu-d-scanning-technology-interaction.html>CMU researchers develop 3D scanning tech...</a></li>
                 <br><li><strong>June 2, 2015</strong> <br><a href=http://phys.org/news/2015-06-team-camera-sensors-pixels.html>Team develops camera that uses sensor...</a></li>
                 
                 </ul>
                 </section>
                 </div>
                 --->
                
                </div>
                </div>
                </div>
                <!-- Footer -->
                
                <!-- Copyright -->
                <div id="copyright">
                    <div class="container">
                        Design: <a href="http://templated.co">TEMPLATED</a><br>Header Image: <a href="images/byeng.jpg">Rufus2010</a>.
                        <i><a rel="nofollow" class="external text" href="http://creativecommons.org/licenses/by-sa/2.5/">(CC Attribution-ShareAlike 2.5)</a></i>
                    </div>
                </div>
                
                </body>
                </html>
